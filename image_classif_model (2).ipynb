{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f7c5d8-b0ec-4d7d-9e64-349dde755d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import cv2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7288d2e-6608-4492-b93a-3c022518db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import keras\n",
    "from keras.applications import ResNet50V2\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa2261-c0d4-43bd-b0d4-cfe80a9385c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaf383b2-5460-4b7f-b77e-5d15534c5260",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocessing the data so that ResNet can accept medical images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6fc4d4c-dedb-448f-b244-fdb821988384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_dicom_files(directory):\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.dcm'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                image = load_and_preprocess_dicom(file_path)\n",
    "                images.append(image)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99eda7ca-fffc-479f-b14f-c1c8f4377ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the  input data from path\n",
    "X_train_path = 'images_train'\n",
    "X_test_path = 'images_test'\n",
    "X_valid_path = 'images_valid'\n",
    "\n",
    "train_X = load_preprocessed_dicom_files(X_train_path)\n",
    "valid_X = load_preprocessed_dicom_files(X_valid_path)\n",
    "test_X  = load_preprocessed_dicom_files(X_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e6418a-197d-4b9a-b9df-b6984c311c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>pe_present_on_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f3cb036d06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f57ffd3883b6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41220fda34a3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13b685b4b14f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be0b7524ffb4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790589</th>\n",
       "      <td>da0ecef50cf5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790590</th>\n",
       "      <td>d74b46c2f2c4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790591</th>\n",
       "      <td>ba71189191ad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790592</th>\n",
       "      <td>f4fdc88f2ace</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790593</th>\n",
       "      <td>f890efd48940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1790594 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SOPInstanceUID  pe_present_on_image\n",
       "0         c0f3cb036d06                    0\n",
       "1         f57ffd3883b6                    0\n",
       "2         41220fda34a3                    0\n",
       "3         13b685b4b14f                    0\n",
       "4         be0b7524ffb4                    0\n",
       "...                ...                  ...\n",
       "1790589   da0ecef50cf5                    0\n",
       "1790590   d74b46c2f2c4                    0\n",
       "1790591   ba71189191ad                    0\n",
       "1790592   f4fdc88f2ace                    0\n",
       "1790593   f890efd48940                    1\n",
       "\n",
       "[1790594 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_csv = 'train.csv'\n",
    "labels_df = pd.read_csv(labels_csv)\n",
    "labels_df = labels_df.iloc[:, [2, 3]]\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2972e5a1-9b25-4e14-946e-fd4f7027fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: DATA\n"
     ]
    }
   ],
   "source": [
    "# Folder path relative to the current working directory in JupyterLab\n",
    "folder_path = 'DATA'  # Adjust as necessary\n",
    "\n",
    "# Verify the correct folder path\n",
    "if os.path.exists(folder_path):\n",
    "    print(\"Directory exists:\", folder_path)\n",
    "else:\n",
    "    print(\"Directory does not exist:\", folder_path)\n",
    "\n",
    "# List to hold filenames without the '.dcim' extension\n",
    "file_names_without_extension = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Print each filename for debugging\n",
    "    #print(\"Found file:\", filename)\n",
    "\n",
    "    # Check for '.dcim' extension considering case sensitivity\n",
    "    if filename.lower().endswith('.dcm'):\n",
    "        # Remove the '.dcim' extension and add to the list\n",
    "        clean_name = filename[:-4]  # This removes the last 5 characters, i.e., '.dcim'\n",
    "        file_names_without_extension.append(clean_name)\n",
    "\n",
    "# Print the list of cleaned names\n",
    "#print(\"Files without .dcim extension:\", file_names_without_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61912f29-98e7-4c0a-9ef5-1c4d538eef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the csv so that it only has the relevant labels\n",
    "valid_uids = file_names_without_extension\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = labels_df[labels_df['SOPInstanceUID'].isin(valid_uids)]\n",
    "\n",
    "# # Optional: Save the filtered DataFrame to a new CSV file\n",
    "# filtered_df.to_csv('filtered_csv_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01525611-ee90-4e6e-90a6-8014561583a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33bd71f7-d338-4f5b-8a7e-ce152d9f535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: images_train\n"
     ]
    }
   ],
   "source": [
    "# Folder path relative to the current working directory in JupyterLab\n",
    "folder_path = 'images_train'  # Adjust as necessary\n",
    "\n",
    "# Verify the correct folder path\n",
    "if os.path.exists(folder_path):\n",
    "    print(\"Directory exists:\", folder_path)\n",
    "else:\n",
    "    print(\"Directory does not exist:\", folder_path)\n",
    "\n",
    "# List to hold filenames without the '.dcim' extension\n",
    "train_ids = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Print each filename for debugging\n",
    "    #print(\"Found file:\", filename)\n",
    "\n",
    "    # Check for '.dcim' extension considering case sensitivity\n",
    "    if filename.lower().endswith('.dcm'):\n",
    "        # Remove the '.dcim' extension and add to the list\n",
    "        clean_name = filename[:-4]  # This removes the last 5 characters, i.e., '.dcim'\n",
    "        train_ids.append(clean_name)\n",
    "\n",
    "# Print the list of cleaned names\n",
    "#print(\"Files without .dcim extension:\", file_names_without_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f70047-e2bb-47e5-90df-9647994eeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = labels_df[labels_df['SOPInstanceUID'].isin(train_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f307cf8-a477-4308-8359-dc0670e45113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: images_test\n"
     ]
    }
   ],
   "source": [
    "# Folder path relative to the current working directory in JupyterLab\n",
    "folder_path = 'images_test'  # Adjust as necessary\n",
    "\n",
    "# Verify the correct folder path\n",
    "if os.path.exists(folder_path):\n",
    "    print(\"Directory exists:\", folder_path)\n",
    "else:\n",
    "    print(\"Directory does not exist:\", folder_path)\n",
    "\n",
    "# List to hold filenames without the '.dcim' extension\n",
    "test_ids = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Print each filename for debugging\n",
    "    #print(\"Found file:\", filename)\n",
    "\n",
    "    # Check for '.dcim' extension considering case sensitivity\n",
    "    if filename.lower().endswith('.dcm'):\n",
    "        # Remove the '.dcim' extension and add to the list\n",
    "        clean_name = filename[:-4]  # This removes the last 5 characters, i.e., '.dcim'\n",
    "        test_ids.append(clean_name)\n",
    "\n",
    "# Print the list of cleaned names\n",
    "#print(\"Files without .dcim extension:\", file_names_without_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9e09943-b9ae-4183-924f-91483d132545",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = labels_df[labels_df['SOPInstanceUID'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62cf79a9-89d3-4cb5-9e02-94894099beff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: images_valida\n"
     ]
    }
   ],
   "source": [
    "# Folder path relative to the current working directory in JupyterLab\n",
    "folder_path = 'images_valida'  # Adjust as necessary\n",
    "\n",
    "# Verify the correct folder path\n",
    "if os.path.exists(folder_path):\n",
    "    print(\"Directory exists:\", folder_path)\n",
    "else:\n",
    "    print(\"Directory does not exist:\", folder_path)\n",
    "\n",
    "# List to hold filenames without the '.dcim' extension\n",
    "valid_ids = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Print each filename for debugging\n",
    "    #print(\"Found file:\", filename)\n",
    "\n",
    "    # Check for '.dcim' extension considering case sensitivity\n",
    "    if filename.lower().endswith('.dcm'):\n",
    "        # Remove the '.dcim' extension and add to the list\n",
    "        clean_name = filename[:-4]  # This removes the last 5 characters, i.e., '.dcim'\n",
    "        valid_ids.append(clean_name)\n",
    "\n",
    "# Print the list of cleaned names\n",
    "#print(\"Files without .dcim extension:\", file_names_without_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d75d3d11-26e8-48f0-8e93-f0007c2f7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = labels_df[labels_df['SOPInstanceUID'].isin(valid_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1adf3-e0f4-486b-a811-fd06582c90b0",
   "metadata": {},
   "source": [
    " ### Here, we instantiate a base model with pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64681b3e-88c9-46b2-b7df-6ef734980e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:49:39.829928: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform XPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-20 21:49:39.829976: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:XPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: XPU, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.ResNet50V2(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fd300-f64a-40fc-9449-d2e46fd47665",
   "metadata": {},
   "source": [
    "### Then, we freeze the base model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645ac8e0-215e-427f-a178-47c9a69ee41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51d759-2f20-41b6-a2ae-50ffa4105389",
   "metadata": {},
   "source": [
    "### Create a new model on top of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f326b63c-875c-46ff-8627-86e326fa93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = keras.Input(shape=(150, 150, 3))\n",
    "# x = base_model(inputs, training=False)\n",
    "# # Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "# x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# # A Dense classifier with a single unit (binary classification)\n",
    "# outputs = keras.layers.Dense(1)(x)\n",
    "# model = keras.Model(inputs, outputs)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(6, activation='swish'),\n",
    "    layers.Dense(6, activation='swish'),\n",
    "    layers.Dense(6, activation='swish'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6569d6-89fd-46e0-a9e7-23c36b87b6d0",
   "metadata": {},
   "source": [
    "### Training the new model on blood clot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0601ac7d-5fec-4372-8cb6-2de77a57881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.00001, # minimium amount of change to count as an improvement\n",
    "    patience= 30, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55684827-efcf-4730-8be0-834df2efc2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a150c70-8c0a-429d-b31b-55f32e67b3dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb549955-99f0-4542-911b-6937d6fd29b0",
   "metadata": {},
   "source": [
    "### Plotting the loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afae763-c46a-45ed-9a80-7d7f99d807fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
    "\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n",
    "print(\"Minimum training loss: {}\".format(history_df['loss'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b7f37-8154-470b-8629-3a8f8001c667",
   "metadata": {},
   "source": [
    "## ! fix  X_train, y_train, X_valid,y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47273f3-45da-4ef7-9558-89221e36a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Train end-to-end. Be careful to stop before you overfit!\n",
    "history = model.fit(\n",
    "    X_train, y_train, # new_dataset             #ADD the CORRECT df's for testing data\n",
    "    validation_data=(X_valid,y_valid), #new_dataset     #ADD the CORRECT df's for validation data\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping], # put callbacks in a list\n",
    "    verbose=0,  # turn off training log\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a94046-b4be-49ae-8c92-1e163faebba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
    "\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n",
    "print(\"Minimum training loss: {}\".format(history_df['loss'].min()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
