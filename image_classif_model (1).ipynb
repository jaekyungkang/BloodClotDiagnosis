{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f7c5d8-b0ec-4d7d-9e64-349dde755d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:49:34.628232: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-20 21:49:34.667941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 21:49:36.735190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-20 21:49:38.141125: I itex/core/wrapper/itex_cpu_wrapper.cc:42] Intel Extension for Tensorflow* AVX512 CPU backend is loaded.\n",
      "2024-04-20 21:49:38.723338: I itex/core/wrapper/itex_gpu_wrapper.cc:35] Intel Extension for Tensorflow* GPU backend is loaded.\n",
      "2024-04-20 21:49:38.755319: W itex/core/ops/op_init.cc:58] Op: _QuantizedMaxPool3D is already registered in Tensorflow\n",
      "2024-04-20 21:49:38.782354: I itex/core/devices/gpu/itex_gpu_runtime.cc:129] Selected platform: Intel(R) Level-Zero\n",
      "2024-04-20 21:49:38.782376: I itex/core/devices/gpu/itex_gpu_runtime.cc:154] number of sub-devices is zero, expose root device.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import cv2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7288d2e-6608-4492-b93a-3c022518db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import keras\n",
    "from keras.applications import ResNet50V2\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf383b2-5460-4b7f-b77e-5d15534c5260",
   "metadata": {},
   "source": [
    "## Preprocessing the data so that ResNet can accept the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02e0f63-ed72-42fb-a6a8-d70dc506c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_dicom(path):\n",
    "    ds = pydicom.dcmread(path)\n",
    "    image = ds.pixel_array\n",
    "\n",
    "    # Normalize to the range 0-255 and convert to uint8\n",
    "    if image.dtype != np.uint8:\n",
    "        image = (np.clip(image, 0, np.max(image)) / np.max(image) * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert grayscale to RGB\n",
    "    if image.ndim == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Resize image to match ResNet input size\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    # Apply preprocessing specific for ResNetV2\n",
    "    image = preprocess_input(image.astype('float32'))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fc4d4c-dedb-448f-b244-fdb821988384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_dicom_files(directory):\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.dcm'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                image = load_and_preprocess_dicom(file_path)\n",
    "                images.append(image)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99eda7ca-fffc-479f-b14f-c1c8f4377ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to use the function\n",
    "dicom_train_directory = 'images_train'\n",
    "dicom_test_directory  = 'images_test'\n",
    "dicom_valid_directory = 'images_valid'\n",
    "\n",
    "train_images = load_preprocessed_dicom_files(dicom_train_directory)\n",
    "test_images  = load_preprocessed_dicom_files(dicom_test_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6418a-197d-4b9a-b9df-b6984c311c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfd1adf3-e0f4-486b-a811-fd06582c90b0",
   "metadata": {},
   "source": [
    " ### Here, we instantiate a base model with pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64681b3e-88c9-46b2-b7df-6ef734980e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:49:39.829928: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform XPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-20 21:49:39.829976: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:XPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: XPU, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.ResNet50V2(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fd300-f64a-40fc-9449-d2e46fd47665",
   "metadata": {},
   "source": [
    "### Then, we freeze the base model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645ac8e0-215e-427f-a178-47c9a69ee41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51d759-2f20-41b6-a2ae-50ffa4105389",
   "metadata": {},
   "source": [
    "### Create a new model on top of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f326b63c-875c-46ff-8627-86e326fa93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6569d6-89fd-46e0-a9e7-23c36b87b6d0",
   "metadata": {},
   "source": [
    "### Training the new model on blood clot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0601ac7d-5fec-4372-8cb6-2de77a57881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.00001, # minimium amount of change to count as an improvement\n",
    "    patience= 30, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55684827-efcf-4730-8be0-834df2efc2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b7f37-8154-470b-8629-3a8f8001c667",
   "metadata": {},
   "source": [
    "## ! fix  X_train, y_train, X_valid,y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a150c70-8c0a-429d-b31b-55f32e67b3dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mX_train\u001b[49m, y_train, \u001b[38;5;66;03m#ADD the CORRECT df's for testing data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_valid,y_valid), \u001b[38;5;66;03m#ADD the CORRECT df's for validation data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      6\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping], \u001b[38;5;66;03m# put callbacks in a list\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,  \u001b[38;5;66;03m# turn off training log\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, #ADD the CORRECT df's for testing data\n",
    "    validation_data=(X_valid,y_valid), #ADD the CORRECT df's for validation data\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping], # put callbacks in a list\n",
    "    verbose=0,  # turn off training log\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb549955-99f0-4542-911b-6937d6fd29b0",
   "metadata": {},
   "source": [
    "### Plotting the loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afae763-c46a-45ed-9a80-7d7f99d807fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)#this creates a new df that keeps track of the training/validation \n",
    "history_df.loc[:, ['loss', 'val_loss']].plot();#this is used to visualize the loss history\n",
    "\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n",
    "print(\"Minimum training loss: {}\".format(history_df['loss'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47273f3-45da-4ef7-9558-89221e36a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Train end-to-end. Be careful to stop before you overfit!\n",
    "model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
